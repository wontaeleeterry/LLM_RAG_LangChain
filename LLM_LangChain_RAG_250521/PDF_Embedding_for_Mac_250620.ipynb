{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef15ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/.DS_Store', './data/constitution_of_Korea.pdf']\n"
     ]
    }
   ],
   "source": [
    "# ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  íŒŒì¼ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "import os\n",
    "\n",
    "def list_files(directory):\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_list.append(os.path.join(root, file))\n",
    "    return file_list\n",
    "\n",
    "# ì§€ì •ëœ ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  íŒŒì¼ëª…ì„ ë¦¬ìŠ¤íŠ¸ë¡œ í˜¸ì¶œ\n",
    "file_names = list_files('./data')\n",
    "print(file_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "effb952e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kv/jhs4vb392hb9h62z_b83xjwm0000gn/T/ipykernel_85942/3679939528.py:9: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings_model = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "), model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', cache_folder=None, model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': True}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# íŒ¨í‚¤ì§€ ê´€ë¦¬ (250620)\n",
    "# conda install conda-forge::langchain-community\n",
    "# conda install conda-forge::sentence-transformers\n",
    "\n",
    "# ë¬¸ì¥ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ê³  ë²¡í„° ì €ì¥ì†Œì— ì €ì¥\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',    # ë‹¤êµ­ì–´ ëª¨ë¸\n",
    "    # model_name='jhgan/ko-sroberta-multitask',  # í•œêµ­ì–´ ëª¨ë¸ - ì—ëŸ¬ ë°œìƒ (250603)\n",
    "    # model_name = 'BAAI/bge-m3',                # ì—ëŸ¬ ë°œìƒ (250603)\n",
    "    model_kwargs={'device':'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings':True},\n",
    ")\n",
    "\n",
    "embeddings_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2637130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "# pip install PyMuPDF   # conda ì„¤ì¹˜ ì‹œ ì—ëŸ¬ ë°œìƒ\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "# conda install conda-forge::langchain-chroma\n",
    "\n",
    "loader = PyMuPDFLoader(file_names[1])       # í´ë” ë‚´ íŒŒì¼ 1ê°œë§Œ ì¡´ì¬ : ì—¬ëŸ¬ ê°œì¼ ê²½ìš°, ìµœì´ˆ 1ê°œ DB ìƒì„± í›„, Add ë°©ì‹ìœ¼ë¡œ ì§„í–‰ (250605)\n",
    "documents = loader.load()\n",
    "\n",
    "# ë§¥ì˜ ê²½ìš° .ds íŒŒì¼ì´ ìƒì„±ë˜ë¯€ë¡œ ì¸ë±ìŠ¤ë¥¼ [1]ë¶€í„° ì´ìš©í•œë‹¤. (250620)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a48512d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=16) \n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# ì„ë² ë”© DB ìƒì„± : íŒŒì¼ë¡œ ì €ì¥í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ìƒˆë¡œ ì‹¤í–‰í•  ê²½ìš° ì´ˆê¸°í™”ë¨ (250605)\n",
    "db_constitution = Chroma.from_documents(\n",
    "    documents=docs, embedding=embeddings_model, collection_name=\"db_constitution\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c7f24e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = db_constitution.similarity_search(\"ëŒ€í†µë ¹\", k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e99d7050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì œ83ì¡° ëŒ€í†µë ¹ì€ êµ­ë¬´ì´ë¦¬ã†êµ­ë¬´ìœ„ì›ã†í–‰ì •ê°ë¶€ì˜ ì¥ ê¸°íƒ€ ë²•ë¥ ì´ ì •í•˜ëŠ” ê³µì‚¬ì˜ ì§ì„ ê²¸í•  ìˆ˜\n",
      "ì—†ë‹¤.\n",
      " \n",
      "ì œ84ì¡° ëŒ€í†µë ¹ì€ ë‚´ë€ ë˜ëŠ” ì™¸í™˜ì˜ ì£„ë¥¼ ë²”í•œ ê²½ìš°ë¥¼ ì œì™¸í•˜ê³ ëŠ” ì¬ì§ì¤‘ í˜•ì‚¬ìƒì˜ ì†Œì¶”ë¥¼ ë°›ì§€\n",
      "ì•„ë‹ˆí•œë‹¤.\n",
      " \n",
      "ì œ85ì¡° ì „ì§ëŒ€í†µë ¹ì˜ ì‹ ë¶„ê³¼ ì˜ˆìš°ì— ê´€í•˜ì—¬ëŠ” ë²•ë¥ ë¡œ ì •í•œë‹¤.\n",
      " \n",
      "                    ì œ2ì ˆ í–‰ì •ë¶€\n",
      "                       ì œ1ê´€ êµ­ë¬´ì´ë¦¬ì™€ êµ­ë¬´ìœ„ì›"
     ]
    }
   ],
   "source": [
    "print(aa[9].page_content, end = \"\")  # ê°œí–‰ ë¬¸ì ì—†ì´ ì¶œë ¥í•˜ê¸° (250620)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ba32f3",
   "metadata": {},
   "source": [
    "#### Local LLM ì‚¬ìš©í•˜ê¸° (250620)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1479e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "# pip install langchain-openai\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"lm-studio\",\n",
    "    # model=\"lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF\",\n",
    "    model = \"lmstudio-community/gemma-2-2b-it-GGUF\",\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()], # ìŠ¤íŠ¸ë¦¼ ì¶œë ¥ ì½œë°±\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "801e8898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ˜„  í•œêµ­ì–´ë¡œ í¸í•˜ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”. ğŸ‘ \n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"{input} í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì¤˜.\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "# response = chain.invoke(\"ì•ˆë…•!\")\n",
    "response = chain.invoke({'input' : \"ì•ˆë…•!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92aa36ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŠ¹ì • ë‚´ìš©ì„ ê²€ìƒ‰í•˜ê³  ê·¸ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ëŠ” ê²½ìš°,\n",
    "\n",
    "# ê²€ìƒ‰ ì¿¼ë¦¬\n",
    "query = 'íƒ„í•µ'    # í‚¤ì›Œë“œì— ëŒ€í•œ ë‚´ìš©ì„ ë¨¼ì € ì¶”ì¶œ\n",
    "\n",
    "# ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì€ ë¬¸ì¥ ì¶”ì¶œ\n",
    "retriever = db_constitution.as_retriever(search_kwargs={'k': 20})\n",
    "docs = retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66ac5ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëŒ€í•œë¯¼êµ­í—Œë²• ì œ73ì¡°ì— ë”°ë¥´ë©´, ì¡°ì•½ì„ ì²´ê²°í•˜ê±°ë‚˜ êµ­ì œì¡°ì§ì— ê´€í•œ ì¡°ì•½ì„ ì²´ê²°í•˜ëŠ” ë“±ì˜ í–‰ìœ„ë¥¼ í†µí•´ ëŒ€í†µë ¹ì´ íƒ„í•µë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
      "\n",
      "**ë” ìì„¸íˆ ì„¤ëª…í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.**\n",
      "\n",
      "* **íƒ„í•µì†Œì¶” ì˜ê²°:**  ëŒ€í†µë ¹ì€ íƒ„í•µ ì†Œì¶” ì˜ê²°ì„ ë°›ì•˜ë‹¤ë©´, ê·¸ ê¶Œí•œí–‰ì‚¬ê°€ ì •ì§€ë©ë‹ˆë‹¤.\n",
      "* **íƒ„í•µì‹¬íŒ:** íƒ„í•µì‹¬íŒì´ ìˆì„ ë•Œê¹Œì§€ ëŒ€í†µë ¹ì˜ ê¶Œí•œ í–‰ì‚¬ëŠ” ì •ì§€ëœë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
      "* **íƒ„í•µ ê²°ì •:**  íƒ„í•µê²°ì •ì€ ê³µì§ìœ¼ë¡œë¶€í„° íŒŒë©´ë˜ì§€ë§Œ, ë¯¼ì‚¬ìƒì´ë‚˜ í˜•ì‚¬ìƒì˜ ì±…ì„ì´ ë©´ì œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt í…œí”Œë¦¿ ìƒì„±\n",
    "template = '''Answer the question based only on the following context:\n",
    "{context} Please answer all the answers in Korean.:\n",
    "\n",
    "Question: {question}\n",
    "'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return '\\n\\n'.join([d.page_content for d in docs])\n",
    "\n",
    "# RAG Chain ì—°ê²°\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Chain ì‹¤í–‰\n",
    "query = \"ëŒ€í†µë ¹ì€ íƒ„í•µë  ìˆ˜ ìˆë‚˜ìš”?\"\n",
    "answer = rag_chain.invoke({'context': (format_docs(docs)), 'question': query}) \n",
    "# ê¸° ì¶œì¶œëœ 20ê°œì˜ context ë²”ìœ„ ë‚´ì—ì„œ questionì— ëŒ€í•œ ì‘ë‹µì„ ì°¾ì„ ê²ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa234d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "torch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
