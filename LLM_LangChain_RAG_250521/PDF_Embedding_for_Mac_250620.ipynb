{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef15ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/.DS_Store', './data/constitution_of_Korea.pdf']\n"
     ]
    }
   ],
   "source": [
    "# 디렉토리 내 모든 파일을 리스트로 변환하는 함수 정의\n",
    "\n",
    "import os\n",
    "\n",
    "def list_files(directory):\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_list.append(os.path.join(root, file))\n",
    "    return file_list\n",
    "\n",
    "# 지정된 디렉토리 내 모든 파일명을 리스트로 호출\n",
    "file_names = list_files('./data')\n",
    "print(file_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "effb952e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kv/jhs4vb392hb9h62z_b83xjwm0000gn/T/ipykernel_85942/3679939528.py:9: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings_model = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "), model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', cache_folder=None, model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': True}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 패키지 관리 (250620)\n",
    "# conda install conda-forge::langchain-community\n",
    "# conda install conda-forge::sentence-transformers\n",
    "\n",
    "# 문장을 임베딩으로 변환하고 벡터 저장소에 저장\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',    # 다국어 모델\n",
    "    # model_name='jhgan/ko-sroberta-multitask',  # 한국어 모델 - 에러 발생 (250603)\n",
    "    # model_name = 'BAAI/bge-m3',                # 에러 발생 (250603)\n",
    "    model_kwargs={'device':'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings':True},\n",
    ")\n",
    "\n",
    "embeddings_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2637130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "# pip install PyMuPDF   # conda 설치 시 에러 발생\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "# conda install conda-forge::langchain-chroma\n",
    "\n",
    "loader = PyMuPDFLoader(file_names[1])       # 폴더 내 파일 1개만 존재 : 여러 개일 경우, 최초 1개 DB 생성 후, Add 방식으로 진행 (250605)\n",
    "documents = loader.load()\n",
    "\n",
    "# 맥의 경우 .ds 파일이 생성되므로 인덱스를 [1]부터 이용한다. (250620)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a48512d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=16) \n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# 임베딩 DB 생성 : 파일로 저장하지 않으므로, 새로 실행할 경우 초기화됨 (250605)\n",
    "db_constitution = Chroma.from_documents(\n",
    "    documents=docs, embedding=embeddings_model, collection_name=\"db_constitution\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c7f24e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = db_constitution.similarity_search(\"대통령\", k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e99d7050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제83조 대통령은 국무총리ㆍ국무위원ㆍ행정각부의 장 기타 법률이 정하는 공사의 직을 겸할 수\n",
      "없다.\n",
      " \n",
      "제84조 대통령은 내란 또는 외환의 죄를 범한 경우를 제외하고는 재직중 형사상의 소추를 받지\n",
      "아니한다.\n",
      " \n",
      "제85조 전직대통령의 신분과 예우에 관하여는 법률로 정한다.\n",
      " \n",
      "                    제2절 행정부\n",
      "                       제1관 국무총리와 국무위원"
     ]
    }
   ],
   "source": [
    "print(aa[9].page_content, end = \"\")  # 개행 문자 없이 출력하기 (250620)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ba32f3",
   "metadata": {},
   "source": [
    "#### Local LLM 사용하기 (250620)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1479e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "# pip install langchain-openai\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"lm-studio\",\n",
    "    # model=\"lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF\",\n",
    "    model = \"lmstudio-community/gemma-2-2b-it-GGUF\",\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()], # 스트림 출력 콜백\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "801e8898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! 😊 무엇을 도와드릴까요? 😄  한국어로 편하게 말씀해주세요. 👍 \n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"{input} 한국어로 답변해줘.\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "# response = chain.invoke(\"안녕!\")\n",
    "response = chain.invoke({'input' : \"안녕!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92aa36ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 내용을 검색하고 그 결과를 참고하는 경우,\n",
    "\n",
    "# 검색 쿼리\n",
    "query = '탄핵'    # 키워드에 대한 내용을 먼저 추출\n",
    "\n",
    "# 가장 유사도가 높은 문장 추출\n",
    "retriever = db_constitution.as_retriever(search_kwargs={'k': 20})\n",
    "docs = retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66ac5ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대한민국헌법 제73조에 따르면, 조약을 체결하거나 국제조직에 관한 조약을 체결하는 등의 행위를 통해 대통령이 탄핵될 수 있습니다.  \n",
      "\n",
      "**더 자세히 설명해 드리겠습니다.**\n",
      "\n",
      "* **탄핵소추 의결:**  대통령은 탄핵 소추 의결을 받았다면, 그 권한행사가 정지됩니다.\n",
      "* **탄핵심판:** 탄핵심판이 있을 때까지 대통령의 권한 행사는 정지된다는 것을 의미합니다.\n",
      "* **탄핵 결정:**  탄핵결정은 공직으로부터 파면되지만, 민사상이나 형사상의 책임이 면제되지 않습니다.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt 템플릿 생성\n",
    "template = '''Answer the question based only on the following context:\n",
    "{context} Please answer all the answers in Korean.:\n",
    "\n",
    "Question: {question}\n",
    "'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return '\\n\\n'.join([d.page_content for d in docs])\n",
    "\n",
    "# RAG Chain 연결\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Chain 실행\n",
    "query = \"대통령은 탄핵될 수 있나요?\"\n",
    "answer = rag_chain.invoke({'context': (format_docs(docs)), 'question': query}) \n",
    "# 기 출출된 20개의 context 범위 내에서 question에 대한 응답을 찾을 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa234d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "torch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
