{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b989fb36",
   "metadata": {},
   "source": [
    "LangChain, RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "778e5896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 출처 : https://wikidocs.net/231393\n",
    "# pip install langchain_community\n",
    "\n",
    "\n",
    "# 웹 로더더\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# 여러 개의 url 지정 가능\n",
    "url1 = \"https://blog.langchain.dev/customers-replit/\"\n",
    "url2 = \"https://blog.langchain.dev/langgraph-v0-2/\"\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(url1, url2),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"article-header\", \"article-content\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb6b0bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://blog.langchain.dev/customers-replit/'}, page_content='\\nReplit is at the forefront of AI innovation with its platform that simplifies writing, running, and collaborating on code for over 30+ million developers. They recently released Replit Agent, which immediately went viral due to the incredible applications people could easily create with this tool.Behind the scenes, Replit Agent has a complex workflow built on LangGraph, which enables a highly custom agentic workflow with a high-degree of control and parallel execution. A major benefit of using LangGraph was the seamless integration with LangSmith, which gave Replit deep visibility into their agent interactions to debug tricky issues.\\xa0The level of complexity required for Replit Agent also pushed the boundaries of LangSmith. The LangChain and Replit teams worked closely together to add functionality to LangSmith that would satisfy their LLM observability needs. Specifically, there were three main areas that we innovated on:Improved performance and scale on large tracesAbility to search and filter within tracesThread view to enable human-in-the loop workflowsImproved performance and scale on large tracesMost other LLMOps solutions monitor individual API requests to LLM providers, offering a limited view of single LLM calls. In contrast, LangSmith from day one has focused on tracing the entire execution flow of an LLM application to provide a more holistic context.\\xa0Tracing is important for agents due to their complex nature. It captures multiple LLM calls as well as other steps (retrieval, running code, etc). This gives you granular visibility into what’s happening, including at the inputs and outputs of each step, in order to understand the agent’s decision-making.\\xa0Replit Agent was a ripe example for advanced tracing needs. Their agentic tool goes beyond simply reviewing and writing code, but also performs a wider range of functions – including planning, creating dev environments, installing dependencies, and deploying applications for users.\\xa0As a result, Replit’s traces were very large - involving hundreds of steps. This posed significant challenges for ingesting data and displaying it in a visually meaningful way.To address this, the LangChain team improved their ingestion to efficiently process and store large volumes of trace data. They also improved LangSmith’s frontend rendering to display long-running agent traces seamlessly.Search and filter within traces to pinpoint issuesLangSmith has always supported search between traces, which allows users to find a single trace among hundreds of thousands based on events or full text search. But as Replit Agent’s traces got longer and longer, the Replit team needed to search within traces for specific events (oftentimes issues reported by alpha testers). This required augmenting existing search capabilities.In response, a new search pattern – searching within traces – was added to LangSmith. Instead of sifting and scrolling call-by-call within a large trace, users could now filter directly on a criteria they cared about (e.g. keywords in the inputs or outputs of a run). This greatly reduced Replit’s time needed to debug agent steps within a trace.Thread view to enable human-in-the-loop workflowsA key differentiator of Replit Agent was its emphasis on human-in-the-loop workflows. Replit Agent intends to be a tool where AI agents can collaborate effectively with human developers, who can come in and edit and correct agent trajectories as needed.With separate agents to perform roles like managing, editing, and verifying generated code,\\xa0 Replit’s agents interacted with users continuously - often over long periods with multiple turns of conversation. However, monitoring these conversational flows was often difficult, as each user session would generate disjoint traces.\\xa0To solve this, LangSmith’s thread view helped collate traces from multiple threads together that were related (i.e. from one conversation). This provided a logical view of all agent-user interactions across a multi-turn conversation, helping Replit better 1) find bottlenecks where users got stuck and 2) pinpoint areas where human intervention could be beneficial.\\xa0ConclusionReplit is pushing the frontier of AI agent monitoring using LangSmith’s powerful observability features. By reducing the effort of loading long, heavy traces, the Replit team has greatly sped up the process of building and scaling complex agents. With faster debugging, improved trace visibility, and better handling of parallel tasks, Replit is setting the standard for AI-driven development.\\t\\n')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]   # 첫 번째 문서 내용 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f2072fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://blog.langchain.dev/langgraph-v0-2/'}, page_content=\"\\nToday, we’re excited to announce the stable release of LangGraph v0.2, which introduces a new ecosystem of LangGraph checkpointer libraries. These simplify the creation and customization of checkpointers, which allows users to build more resilient LLM applications with smooth session memory, robust error recovery, and human-in-the-loop features.Why we built LangGraph v0.2One of the key pillars of LangGraph is its built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at each step, enabling several powerful capabilities, including:Session memory: Store history (checkpoints) of user interactions and resume from a saved checkpoint in follow up interactionsError recovery: Recover from failures at any given step in the graph execution by continuing from the last successful step checkpointHuman-in-the-loop: Implement tool approval, wait for human input, edit agent actions and moreTime travel: Edit graph state at any point in the history of execution and create an alternative execution from that point in time (i.e. fork the thread)Since the early days of LangGraph, we’ve designed checkpointing to be database-agnostic, allowing users to implement their own checkpointer adapters for their database of choice.Since the LangGraph v0.1 release, we've seen a lot of interest from the community in creating checkpointers for many popular databases like Postgres, Redis, and MongoDB. However, there was no clear blueprint for the users to implement their own, custom checkpointers.New checkpointer libraries in LangGraphWith LangGraph v0.2, we’re making it easier to create new checkpointers. We’ve also laid the foundation to foster a community-maintained ecosystem of checkpointer implementations.We now have a suite of new, dedicated checkpointer libraries:langgraph_checkpoint : The base interface for checkpointer savers (BaseCheckpointSaver ) and serialization/deserialization interface (SerializationProtocol). Includes in-memory checkpointer implementation (MemorySaver) for experimentation.langgraph_checkpoint_sqlite : An implementation of LangGraph checkpointer that uses SQLite database. Ideal for experimentation and local workflows.langgraph_checkpoint_postgres : Our advanced checkpointer that we wrote and optimized for Postgres in LangGraph Cloud is now open-sourced and available to the community to build upon. Ideal for using in production.Checkpointer implementations can be used interchangeably, which lets users tailor their stateful LangGraph applications to their custom needs.LangGraph Postgres Checkpointer for production-ready appslanggraph_checkpoint_postgres implementation can serve as a blueprint for community members to implement their own optimized, production-ready checkpointers for their favorite database. Postgres checkpointer implements a number of optimizations both on the write-, as well as read-side.Write-side optimizations:We're making use of Postgres pipeline mode to reduce database roundtripsWe're storing each channel value separately and versioned so that each new checkpoint only stores the values that changed.Read-side optimizations:We're making use of a cursor for the list endpoint in order to efficiently fetch long thread histories when needed.Getting started on LangGraph v0.2Since LangGraph checkpointer libraries are implemented as namespace packages, you can import checkpointer interfaces and implementations the same way as before, using:from langgraph.checkpoint.base import BaseCheckpointSaverfrom langgraph.checkpoint.memory import MemorySaverfrom langgraph.checkpoint.sqlite import SqliteSaverfrom langgraph.checkpoint.postgres import PostgresSaverSince SQLite and Postgres checkpointers are provided via separate libraries, you will need to install them using pip install langgraph-checkpoint-sqlite or pip install langgraph-checkpoint-postgres.LangGraph checkpoint libraries will follow semantic versioning (starting with current release of 1.0), and any breaking changes in the main library will result in a major version bump for those libraries. For example, the next breaking change in langgraph_checkpoint will result in 2.0 version, and you can expect the checkpointer implementations (e.g.,langgraph_checkpoint_sqlite) to also be updated to 2.0 to follow that change.To get started, follow this guide on how to use checkpointers in LangGraph. You can also check out our documentation, including a reference and overview of checkpointers. Run agents at scale with LangGraph CloudTo complement the LangGraph framework, we also have a new runtime, LangGraph Cloud, which provides infrastructure purpose-built for deploying agents at scale.LangGraph Cloud does the heavy lifting for your agentic application, removing the maintenance work for custom checkpointers while adding fault-tolerant scalability. It gracefully manages horizontally-scaling task queues, servers, and includes our robust Postgres checkpointer out-of-the-box to help you handle many concurrent users and efficiently store large states and threads.In addition, LangGraph Cloud supports real-world interaction patterns beyond streaming and human-in-the-loop. These include double-texting to handle new user inputs on currently-running threads of the graph, async background jobs for long-running tasks, and cron jobs.Lastly, you can easily deploy your agentic app and collaborate in LangGraph Studio, a playground-like space for visualizing and debugging agent trajectories, with LangGraph Cloud. The LangGraph Studio desktop app is now also available for all LangSmith users to try for free.LangGraph Cloud is now available in beta for all LangSmith users on Plus or Enterprise plans. Try it out today for free by signing up for LangSmith.Additional changes in LangGraph v0.2LangGraph v0.2 contains many improvements, and we've designed it to be largely backwards compatible. Below is a list of breaking changes and deprecations in this latest version.Breaking changesLangGraph v0.2 introduces several breaking changes:thread_ts and parent_ts have been renamed to checkpoint_id and parent_checkpoint_id , respectively (via langgraph_checkpoint==1.0.0).Note: LangGraph checkpointers still recognize thread_ts if passed via config and treat it as checkpoint_idRe-exported imports like from langgraph.checkpoint import BaseCheckpointSaver are no longer possible due to the use of namespace packages. Instead, use from langgraph.checkpoint.base import BaseCheckpointSaverSQLite checkpointers have been moved to a separate library, so you’ll need to install them separately using pip install langgraph-checkpoint-sqliteDeprecationsIn LangGraph v0.2, we've removed:langgraph.prebuilt.chat_agent_executor.create_function_calling_executor . We recommend you use langgraph.prebuilt.chat_agent_executor.create_react_agent instead.langgraph.prebuilt.agent_executor . We recommend you use langgraph.prebuilt.chat_agent_executor.create_react_agent instead.ConclusionWe are incredibly grateful to our community and users for pushing us and building with LangGraph to improve agent reliability. We hope that with LangGraph v0.2, you’ll find it easier to build and maintain your own checkpointer implementations– and we’re excited to see all the apps that you create.As you try out LangGraph v0.2, we'd love to hear your feedback at hello@langchain.dev. You can also learn more from these additional resources:LangGraph docsLangGraph webpage (with FAQs)\\n\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9577fcf",
   "metadata": {},
   "source": [
    "LangChain 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee5d23bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 문자로 구분하여 분할\n",
    "\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator='',\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 100,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_text(docs[0].page_content)\n",
    "\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5412df72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a685b0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Replit is at the forefront of AI innovation with its platform that simplifies writing, running, and collaborating on code for over 30+ million developers. They recently released Replit Agent, which immediately went viral due to the incredible applications people could easily create with this tool.Behind the scenes, Replit Agent has a complex workflow built on LangGraph, which enables a highly custom agentic workflow with a high-degree of control and parallel execution. A major benefit of using'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3876a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대규모 언어 모델(LLM)을 사용할 때 모델이 처리할 수 있는 토큰 수의 한계 \n",
    "# 입력 데이터를 모델의 제한을 초과하지 않도록 적절히 분할하는 것이 중요\n",
    "# 이때, LLM 모델에 적용되는 토크나이저를 기준으로 텍스트를 토큰을 분할하고,\n",
    "# 이 토큰들의 수를 기준으로 텍스트를 Chunk 단위로 나누면 모델 입력 토큰 수를 조절할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d05d7044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 참고 : https://littlefoxdiary.tistory.com/114\n",
    "# LangChain에서 문서를 분할할 수 있는 여러가지 TextSplitter\n",
    "\n",
    "# pip install langchain\n",
    "# pip install tiktoken : 에러 메세지 해결\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size = 600,\n",
    "    chunk_overlap = 200,\n",
    "    encoding_name = 'cl100k_base'\n",
    ")\n",
    "\n",
    "docs_sp = text_splitter.split_documents(docs)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "562fed3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://blog.langchain.dev/customers-replit/'}, page_content='Replit is at the forefront of AI innovation with its platform that simplifies writing, running, and collaborating on code for over 30+ million developers. They recently released Replit Agent, which immediately went viral due to the incredible applications people could easily create with this tool.Behind the scenes, Replit Agent has a complex workflow built on LangGraph, which enables a highly custom agentic workflow with a high-degree of control and parallel execution. A major benefit of using LangGraph was the seamless integration with LangSmith, which gave Replit deep visibility into their agent interactions to debug tricky issues.\\xa0The level of complexity required for Replit Agent also pushed the boundaries of LangSmith. The LangChain and Replit teams worked closely together to add functionality to LangSmith that would satisfy their LLM observability needs. Specifically, there were three main areas that we innovated on:Improved performance and scale on large tracesAbility to search and filter within tracesThread view to enable human-in-the loop workflowsImproved performance and scale on large tracesMost other LLMOps solutions monitor individual API requests to LLM providers, offering a limited view of single LLM calls. In contrast, LangSmith from day one has focused on tracing the entire execution flow of an LLM application to provide a more holistic context.\\xa0Tracing is important for agents due to their complex nature. It captures multiple LLM calls as well as other steps (retrieval, running code, etc). This gives you granular visibility into what’s happening, including at the inputs and outputs of each step, in order to understand the agent’s decision-making.\\xa0Replit Agent was a ripe example for advanced tracing needs. Their agentic tool goes beyond simply reviewing and writing code, but also performs a wider range of functions – including planning, creating dev environments, installing dependencies, and deploying applications for users.\\xa0As a result, Replit’s traces were very large - involving hundreds of steps. This posed significant challenges for ingesting data and displaying it in a visually meaningful way.To address this, the LangChain team improved their ingestion to efficiently process and store large volumes of trace data. They also improved LangSmith’s frontend rendering to display long-running agent traces seamlessly.Search and filter within traces to pinpoint issuesLangSmith has always supported search between traces, which allows users to find a single trace among hundreds of thousands based on events or full text search. But as Replit Agent’s traces got longer and longer, the Replit team needed to search within traces for specific events (oftentimes issues reported by alpha testers). This required augmenting existing search capabilities.In response, a new search pattern – searching within traces – was added to LangSmith. Instead of sifting and scrolling call-by-call within a large trace, users could now filter directly on a criteria they cared about (e.g. keywords in the inputs or outputs of a run). This greatly reduced Replit’s time needed to debug agent steps within a trace.Thread view to enable human-in-the-loop workflowsA key differentiator of Replit Agent was its emphasis on human-in-the-loop workflows. Replit Agent intends to be a tool where AI agents can collaborate effectively with human developers, who can come in and edit and correct agent trajectories as needed.With separate agents to perform roles like managing, editing, and verifying generated code,\\xa0 Replit’s agents interacted with users continuously - often over long periods with multiple turns of conversation. However, monitoring these conversational flows was often difficult, as each user session would generate disjoint traces.\\xa0To solve this, LangSmith’s thread view helped collate traces from multiple threads together that were related (i.e. from one conversation). This provided a logical view of all agent-user interactions across a multi-turn conversation, helping Replit better 1) find bottlenecks where users got stuck and 2) pinpoint areas where human intervention could be beneficial.\\xa0ConclusionReplit is pushing the frontier of AI agent monitoring using LangSmith’s powerful observability features. By reducing the effort of loading long, heavy traces, the Replit team has greatly sped up the process of building and scaling complex agents. With faster debugging, improved trace visibility, and better handling of parallel tasks, Replit is setting the standard for AI-driven development.')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_sp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "114276b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://blog.langchain.dev/langgraph-v0-2/'}, page_content=\"Today, we’re excited to announce the stable release of LangGraph v0.2, which introduces a new ecosystem of LangGraph checkpointer libraries. These simplify the creation and customization of checkpointers, which allows users to build more resilient LLM applications with smooth session memory, robust error recovery, and human-in-the-loop features.Why we built LangGraph v0.2One of the key pillars of LangGraph is its built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at each step, enabling several powerful capabilities, including:Session memory: Store history (checkpoints) of user interactions and resume from a saved checkpoint in follow up interactionsError recovery: Recover from failures at any given step in the graph execution by continuing from the last successful step checkpointHuman-in-the-loop: Implement tool approval, wait for human input, edit agent actions and moreTime travel: Edit graph state at any point in the history of execution and create an alternative execution from that point in time (i.e. fork the thread)Since the early days of LangGraph, we’ve designed checkpointing to be database-agnostic, allowing users to implement their own checkpointer adapters for their database of choice.Since the LangGraph v0.1 release, we've seen a lot of interest from the community in creating checkpointers for many popular databases like Postgres, Redis, and MongoDB. However, there was no clear blueprint for the users to implement their own, custom checkpointers.New checkpointer libraries in LangGraphWith LangGraph v0.2, we’re making it easier to create new checkpointers. We’ve also laid the foundation to foster a community-maintained ecosystem of checkpointer implementations.We now have a suite of new, dedicated checkpointer libraries:langgraph_checkpoint : The base interface for checkpointer savers (BaseCheckpointSaver ) and serialization/deserialization interface (SerializationProtocol). Includes in-memory checkpointer implementation (MemorySaver) for experimentation.langgraph_checkpoint_sqlite : An implementation of LangGraph checkpointer that uses SQLite database. Ideal for experimentation and local workflows.langgraph_checkpoint_postgres : Our advanced checkpointer that we wrote and optimized for Postgres in LangGraph Cloud is now open-sourced and available to the community to build upon. Ideal for using in production.Checkpointer implementations can be used interchangeably, which lets users tailor their stateful LangGraph applications to their custom needs.LangGraph Postgres Checkpointer for production-ready appslanggraph_checkpoint_postgres implementation can serve as a blueprint for community members to implement their own optimized, production-ready checkpointers for their favorite database. Postgres checkpointer implements a number of optimizations both on the write-, as well as read-side.Write-side optimizations:We're making use of Postgres pipeline mode to reduce database roundtripsWe're storing each channel value separately and versioned so that each new checkpoint only stores the values that changed.Read-side optimizations:We're making use of a cursor for the list endpoint in order to efficiently fetch long thread histories when needed.Getting started on LangGraph v0.2Since LangGraph checkpointer libraries are implemented as namespace packages, you can import checkpointer interfaces and implementations the same way as before, using:from langgraph.checkpoint.base import BaseCheckpointSaverfrom langgraph.checkpoint.memory import MemorySaverfrom langgraph.checkpoint.sqlite import SqliteSaverfrom langgraph.checkpoint.postgres import PostgresSaverSince SQLite and Postgres checkpointers are provided via separate libraries, you will need to install them using pip install langgraph-checkpoint-sqlite or pip install langgraph-checkpoint-postgres.LangGraph checkpoint libraries will follow semantic versioning (starting with current release of 1.0), and any breaking changes in the main library will result in a major version bump for those libraries. For example, the next breaking change in langgraph_checkpoint will result in 2.0 version, and you can expect the checkpointer implementations (e.g.,langgraph_checkpoint_sqlite) to also be updated to 2.0 to follow that change.To get started, follow this guide on how to use checkpointers in LangGraph. You can also check out our documentation, including a reference and overview of checkpointers. Run agents at scale with LangGraph CloudTo complement the LangGraph framework, we also have a new runtime, LangGraph Cloud, which provides infrastructure purpose-built for deploying agents at scale.LangGraph Cloud does the heavy lifting for your agentic application, removing the maintenance work for custom checkpointers while adding fault-tolerant scalability. It gracefully manages horizontally-scaling task queues, servers, and includes our robust Postgres checkpointer out-of-the-box to help you handle many concurrent users and efficiently store large states and threads.In addition, LangGraph Cloud supports real-world interaction patterns beyond streaming and human-in-the-loop. These include double-texting to handle new user inputs on currently-running threads of the graph, async background jobs for long-running tasks, and cron jobs.Lastly, you can easily deploy your agentic app and collaborate in LangGraph Studio, a playground-like space for visualizing and debugging agent trajectories, with LangGraph Cloud. The LangGraph Studio desktop app is now also available for all LangSmith users to try for free.LangGraph Cloud is now available in beta for all LangSmith users on Plus or Enterprise plans. Try it out today for free by signing up for LangSmith.Additional changes in LangGraph v0.2LangGraph v0.2 contains many improvements, and we've designed it to be largely backwards compatible. Below is a list of breaking changes and deprecations in this latest version.Breaking changesLangGraph v0.2 introduces several breaking changes:thread_ts and parent_ts have been renamed to checkpoint_id and parent_checkpoint_id , respectively (via langgraph_checkpoint==1.0.0).Note: LangGraph checkpointers still recognize thread_ts if passed via config and treat it as checkpoint_idRe-exported imports like from langgraph.checkpoint import BaseCheckpointSaver are no longer possible due to the use of namespace packages. Instead, use from langgraph.checkpoint.base import BaseCheckpointSaverSQLite checkpointers have been moved to a separate library, so you’ll need to install them separately using pip install langgraph-checkpoint-sqliteDeprecationsIn LangGraph v0.2, we've removed:langgraph.prebuilt.chat_agent_executor.create_function_calling_executor . We recommend you use langgraph.prebuilt.chat_agent_executor.create_react_agent instead.langgraph.prebuilt.agent_executor . We recommend you use langgraph.prebuilt.chat_agent_executor.create_react_agent instead.ConclusionWe are incredibly grateful to our community and users for pushing us and building with LangGraph to improve agent reliability. We hope that with LangGraph v0.2, you’ll find it easier to build and maintain your own checkpointer implementations– and we’re excited to see all the apps that you create.As you try out LangGraph v0.2, we'd love to hear your feedback at hello@langchain.dev. You can also learn more from these additional resources:LangGraph docsLangGraph webpage (with FAQs)\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_sp[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cf5bf8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49449ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 파일 로더\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('history.txt')\n",
    "data = loader.load()\n",
    "\n",
    "print(type(data))\n",
    "print(len(data)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb8634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 폴더 순회\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "files = glob(os.path.join('./', '*.txt'))\n",
    "files\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(path='./', glob='*.txt', loader_cls=TextLoader)\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f078c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv 파일\n",
    "\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='한국주택금융공사_주택금융관련_지수_20160101.csv', encoding='cp949')\n",
    "data = loader.load()\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf805e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf 문서 \n",
    "# https://wikidocs.net/231565\n",
    "\n",
    "## 문서 페이지 별 로드\n",
    "## 형식이 없는 pdf 문서 로드\n",
    "## 문서의 메타 데이터를 상세하게 추출 - 파일의 경로, 페이지, 정보, 키워드 등 상세 정보\n",
    "## 온라인 문서 로드\n",
    "## 특정 폴더 내부의 문서 로드"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
